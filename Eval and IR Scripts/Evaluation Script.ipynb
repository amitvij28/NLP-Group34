{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8628931,"sourceType":"datasetVersion","datasetId":5159395},{"sourceId":8639063,"sourceType":"datasetVersion","datasetId":5173626},{"sourceId":8671891,"sourceType":"datasetVersion","datasetId":5197287},{"sourceId":8693420,"sourceType":"datasetVersion","datasetId":5213213},{"sourceId":8693896,"sourceType":"datasetVersion","datasetId":5213440},{"sourceId":8694168,"sourceType":"datasetVersion","datasetId":5213765},{"sourceId":8694236,"sourceType":"datasetVersion","datasetId":5213727},{"sourceId":8694238,"sourceType":"datasetVersion","datasetId":5213820},{"sourceId":183550923,"sourceType":"kernelVersion"},{"sourceId":183551117,"sourceType":"kernelVersion"},{"sourceId":62962,"sourceType":"modelInstanceVersion","modelInstanceId":52540},{"sourceId":63126,"sourceType":"modelInstanceVersion","modelInstanceId":52654},{"sourceId":63130,"sourceType":"modelInstanceVersion","modelInstanceId":52656}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:14:53.878185Z","iopub.execute_input":"2024-06-14T19:14:53.878499Z","iopub.status.idle":"2024-06-14T19:14:53.918239Z","shell.execute_reply.started":"2024-06-14T19:14:53.878467Z","shell.execute_reply":"2024-06-14T19:14:53.917376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Script Template :\n\nThis is the template we used for evaluating the models for different experiments. For each experiment, we modify the test dataset and models accordingly","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nimport torch\nimport logging\nlogging.basicConfig(level=logging.ERROR)\n# If there's a GPU available...\nif torch.cuda.is_available():\n\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:14:53.920640Z","iopub.execute_input":"2024-06-14T19:14:53.921061Z","iopub.status.idle":"2024-06-14T19:14:53.927631Z","shell.execute_reply.started":"2024-06-14T19:14:53.921031Z","shell.execute_reply":"2024-06-14T19:14:53.926730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:14:53.928665Z","iopub.execute_input":"2024-06-14T19:14:53.928952Z","iopub.status.idle":"2024-06-14T19:15:06.336334Z","shell.execute_reply.started":"2024-06-14T19:14:53.928928Z","shell.execute_reply":"2024-06-14T19:15:06.335267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install aiohttp","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:15:06.337878Z","iopub.execute_input":"2024-06-14T19:15:06.338196Z","iopub.status.idle":"2024-06-14T19:15:18.666340Z","shell.execute_reply.started":"2024-06-14T19:15:06.338168Z","shell.execute_reply":"2024-06-14T19:15:18.665127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open(\"/kaggle/input/ollama-ranked/ollama/ollama_test_subq_quantemp_bm25.json\") as f:\n    test_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:15:18.685124Z","iopub.execute_input":"2024-06-14T19:15:18.685403Z","iopub.status.idle":"2024-06-14T19:15:18.737423Z","shell.execute_reply.started":"2024-06-14T19:15:18.685380Z","shell.execute_reply":"2024-06-14T19:15:18.736658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For bart based models\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader, Dataset\n# from transformers import AutoTokenizer\n# from sklearn.metrics import classification_report\n\n# with open(\"/kaggle/input/testclaimsnlp/test_claims_quantemp.json\") as f:\n#     test_data_orig = json.load(f)\n\n# # class MultiClassClassifier(nn.Module):\n# #     def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n# #         super().__init__()\n\n# #         self.roberta = AutoModel.from_pretrained(bert_model_path, output_hidden_states=True, output_attentions=True)\n# #         self.dropout = nn.Dropout(dropout)\n# #         self.mlp = nn.Sequential(\n# #             nn.Linear(hidden_dim, mlp_dim),\n# #             nn.ReLU(),\n# #             nn.Linear(mlp_dim, labels_count)\n# #         )\n# #         if freeze_bert:\n# #             print(\"Freezing layers\")\n# #             for param in self.roberta.parameters():\n# #                 param.requires_grad = False\n\n# #     def forward(self, tokens, masks):\n# #         outputs = self.roberta(tokens, attention_mask=masks)\n# #         pooled_output = outputs[\"last_hidden_state\"][:, 0]  # [CLS] token representation\n# #         dropout_output = self.dropout(pooled_output)\n# #         mlp_output = self.mlp(dropout_output)\n# #         return mlp_output\n# from torch import nn\n# class MultiClassClassifier(nn.Module):\n#     def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n#         super().__init__()\n\n#         self.roberta = AutoModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n#         self.dropout = nn.Dropout(dropout)\n#         self.mlp = nn.Sequential(\n#             nn.Linear(hidden_dim, mlp_dim),\n#             nn.ReLU(),\n#             nn.Linear(mlp_dim, labels_count)\n#         )\n\n#         if freeze_bert:\n#             print(\"Freezing layers\")\n#             for param in self.roberta.parameters():\n#                 param.requires_grad = False\n\n#     def forward(self, tokens, masks):\n#         output = self.roberta(tokens, attention_mask=masks)\n#         # Extract the last hidden state\n#         hidden_states = output.last_hidden_state\n#         # Use the hidden state of the first token (usually [CLS])\n#         pooled_output = hidden_states[:, 0, :]\n#         # Apply dropout\n#         pooled_output = self.dropout(pooled_output)\n#         # Pass through the MLP\n#         mlp_output = self.mlp(pooled_output)\n#         return mlp_output\n\n\n\n# # Initialize the model\n# from transformers import AutoTokenizer,AutoModel\n\n# # Load the BERT tokenizer.\n# print('Loading BERT tokenizer...')\n# tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli', do_lower_case=True)\n# # bert_model_path = '/kaggle/input/mathrobertaquantemp/transformers/trained-large-roberta-mnli/1/model_roberta_large_oracle'\n# num_classes = 3  # Set the number of classes based on your task\n# model =   MultiClassClassifier('facebook/bart-large-mnli', num_classes, 1024,768,140,dropout=0.1,freeze_bert=False)\n# # Load the weights\n# checkpoint_path = '/kaggle/input/d/aratrikad/bart-mnli/BART-large-MNLI-trained/model_weights'\n# model.load_state_dict(torch.load(checkpoint_path))\n\n# # Ensure the model is on the GPU\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# # Define a custom dataset class for the test data\n# class CustomDataset(Dataset):\n#     def __init__(self, data, tokenizer, max_len):\n#         self.data = data\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n\n#     def __len__(self):\n#         return len(self.data)\n\n#     def __getitem__(self, idx):\n#         item = self.data[idx]\n#         claim = item['claim']\n# #         print(\"claim\")\n#         concatenated_evidence = \" \".join(item['retrieved_evidence'])\n# #         print(concatenated_evidence)\n#         label_mapping = {\n#                 \"True\": 2,\n#                 \"False\": 1,\n#                 \"Conflicting\": 0\n#             }\n#         feature = \"[Claim]:\"+claim+\"[Evidences]:\"+concatenated_evidence\n        \n#         encoding = self.tokenizer.encode_plus(\n#             feature,\n#             add_special_tokens = True,\n#             max_length=self.max_len,\n#             padding='max_length',\n#             truncation=True,\n#             return_attention_mask = True,   # Construct attn. masks.\n#             return_tensors = 'pt',\n#         )\n        \n#         return {\n#             'input_ids': encoding['input_ids'].flatten(),\n#             'attention_mask': encoding['attention_mask'].flatten(),\n#             'label': torch.tensor(label_mapping[item['label']] ,dtype=torch.long)\n#         }\n\n# # Load the test data\n# with open('/kaggle/input/ranked-test-evidence/ranked_evidence_test.json', 'r') as f:\n#     test_data = json.load(f)\n\n\n# # Tokenizer\n# # tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\n# max_len = 256 # Define the max length according to your needs\n\n# # Prepare the test dataset and dataloader\n# test_dataset = CustomDataset(test_data, tokenizer, max_len)\n# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n# # print(test_dataset)\n\n# # Evaluation function\n# def evaluate(model, dataloader, device):\n#     model.eval()\n#     predictions = []\n#     true_labels = []\n    \n#     with torch.no_grad():\n#         for batch in dataloader:\n# #             print(\"Next batch\")\n           \n#             input_ids = batch['input_ids'].to(device)\n# #             print('-------------------')\n# #             print(len(batch['input_ids']))\n            \n#             attention_mask = batch['attention_mask'].to(device)\n#             labels = batch['label'].to(device)\n#             with torch.no_grad():\n#                 outputs = model(input_ids, attention_mask)\n# #             print(outputs)\n#             _, preds = torch.max(outputs, dim=1)\n\n#             predictions.extend(preds.cpu().numpy())\n# #             print(predictions)\n#             true_labels.extend(labels.cpu().numpy())\n            \n# #     print(true_labels)\n# #     print(predictions)\n#     mismatched_entries = []\n\n#     # Loop through both arrays\n#     for i in range(len(true_labels)):\n#         if true_labels[i] != predictions[i]:\n#             test = test_data[i]\n#             entry = test_data_orig[i]\n#             entry[\"doc\"] = test[\"retrieved_evidence\"]\n#             reverse_label_mapping = {\n#                 2: \"True\",\n#                 1:\"False\",\n#                 0:\"Conflicting\"\n#             }\n#             entry[\"predicted\"] = reverse_label_mapping[predictions[i]]\n#             mismatched_entries.append(entry)\n# #             print(entry)\n#     mismatched_entries_dict = {}\n#     for index, element in enumerate(mismatched_entries):\n# #         print(mismatched_entries[index])\n#         mismatched_entries_dict[index] = element\n    \n#     with open('/kaggle/working/qualitative_result_mismatch_mathroberta_mnli_exp1_final.json', 'w') as f:\n#         json.dump(mismatched_entries_dict, f)\n    \n    \n#     return true_labels, predictions\n\n# # Evaluate the model\n# true_labels, predictions = evaluate(model, test_dataloader, device)\n\n\n# # Calculate and print classification report\n# report = classification_report(true_labels, predictions, target_names=[ 'Conflicting', 'False', 'True'])\n# print(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:15:18.738683Z","iopub.execute_input":"2024-06-14T19:15:18.738991Z","iopub.status.idle":"2024-06-14T19:15:18.750083Z","shell.execute_reply.started":"2024-06-14T19:15:18.738967Z","shell.execute_reply":"2024-06-14T19:15:18.749168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Roberta\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import classification_report\n\nwith open(\"/kaggle/input/testclaimsnlp/test_claims_quantemp.json\") as f:\n    test_data_orig = json.load(f)\n\nfrom torch import nn\nclass MultiClassClassifier(nn.Module):\n    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n        super().__init__()\n\n        self.roberta = AutoModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n        self.dropout = nn.Dropout(dropout)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, mlp_dim),\n            nn.ReLU(),\n            # nn.Linear(mlp_dim, mlp_dim),\n            # # nn.ReLU(),\n            # # nn.Linear(mlp_dim, mlp_dim),\n            # nn.ReLU(),\n            nn.Linear(mlp_dim, labels_count)\n        )\n        # self.softmax = nn.LogSoftmax(dim=1)\n        if freeze_bert:\n            print(\"Freezing layers\")\n            for param in self.roberta.parameters():\n                param.requires_grad = False\n\n    def forward(self, tokens, masks):\n        outputs = self.roberta(tokens, attention_mask=masks)\n        # For classification, using the pooler output (or the first token representation)\n        # If using the pooler output, make sure your model supports it and it matches hidden_dim\n        pooled_output = outputs[\"last_hidden_state\"][:, 0]  # [CLS] token representation\n        dropout_output = self.dropout(pooled_output)\n        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n        # concat_output = self.dropout(concat_output)\n        mlp_output = self.mlp(dropout_output)\n        # proba = self.sigmoid(mlp_output)\n        # proba = self.softmax(mlp_output)\n\n        return mlp_output\n\n# Initialize the model\nfrom transformers import AutoTokenizer,AutoModel\n\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained('roberta-large-mnli', do_lower_case=True)\n# bert_model_path = '/kaggle/input/mathrobertaquantemp/transformers/trained-large-roberta-mnli/1/model_roberta_large_oracle'\nnum_classes = 3  # Set the number of classes based on your task\nmodel =   MultiClassClassifier('/kaggle/input/math-roberta/transformers/math-roberta1/1/Math Roberta', num_classes, 1024,768,140,dropout=0.1,freeze_bert=False)\n# Load the weights\ncheckpoint_path = '/kaggle/input/math-roberta-claim-decomp-trained/MathRoberta-MNLI-ClaimDecomp-trained/model_weights'\nmodel.load_state_dict(torch.load(checkpoint_path))\n\n# Ensure the model is on the GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define a custom dataset class for the test data\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        claim = item['claim']\n#         print(\"claim\")\n        concatenated_evidence = \" \".join(item['retrieved_evidence'])\n#         print(concatenated_evidence)\n        label_mapping = {\n                \"True\": 2,\n                \"False\": 1,\n                \"Conflicting\": 0\n            }\n        feature = \"[Claim]:\"+claim+\"[Evidences]:\"+concatenated_evidence\n        \n        encoding = self.tokenizer.encode_plus(\n            feature,\n            add_special_tokens = True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask = True,   # Construct attn. masks.\n            return_tensors = 'pt',\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label_mapping[item['label']] ,dtype=torch.long)\n        }\n\n# Load the test data\nwith open('/kaggle/input/ollama-ranked/ollama/ollama_test_subq_quantemp_bm25.json', 'r') as f:\n    test_data = json.load(f)\n\n\n# Tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\nmax_len = 256 # Define the max length according to your needs\n\n# Prepare the test dataset and dataloader\ntest_dataset = CustomDataset(test_data, tokenizer, max_len)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n# print(test_dataset)\n\n# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n#             print(\"Next batch\")\n           \n            input_ids = batch['input_ids'].to(device)\n#             print('-------------------')\n#             print(len(batch['input_ids']))\n            \n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            with torch.no_grad():\n                outputs = model(input_ids, attention_mask)\n#             print(outputs)\n            _, preds = torch.max(outputs, dim=1)\n\n            predictions.extend(preds.cpu().numpy())\n#             print(predictions)\n            true_labels.extend(labels.cpu().numpy())\n            \n#     print(true_labels)\n#     print(predictions)\n    mismatched_entries = []\n\n    # Loop through both arrays\n    for i in range(len(true_labels)):\n        if true_labels[i] != predictions[i]:\n            test = test_data[i]\n            entry = test_data_orig[i]\n            entry[\"doc\"] = test[\"retrieved_evidence\"]\n            reverse_label_mapping = {\n                2: \"True\",\n                1:\"False\",\n                0:\"Conflicting\"\n            }\n            entry[\"predicted\"] = reverse_label_mapping[predictions[i]]\n            mismatched_entries.append(entry)\n#             print(entry)\n    mismatched_entries_dict = {}\n    for index, element in enumerate(mismatched_entries):\n#         print(mismatched_entries[index])\n        mismatched_entries_dict[index] = element\n    \n    with open('/kaggle/working/qualitative_result_mismatch_mathroberta_ollama_mnli_exp1_final.json', 'w') as f:\n        json.dump(mismatched_entries_dict, f)\n    \n    \n    return true_labels, predictions\n\n# Evaluate the model\ntrue_labels, predictions = evaluate(model, test_dataloader, device)\n\n\n# Calculate and print classification report\nreport = classification_report(true_labels, predictions, target_names=[ 'Conflicting', 'False', 'True'])\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:15:18.751340Z","iopub.execute_input":"2024-06-14T19:15:18.751638Z","iopub.status.idle":"2024-06-14T19:16:43.080762Z","shell.execute_reply.started":"2024-06-14T19:15:18.751592Z","shell.execute_reply":"2024-06-14T19:16:43.079805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\nw_f1 =  f1_score(true_labels, predictions, average='weighted')\nprint(f'M-F1: {macro_f1} ')\nprint(f'W-F1: {w_f1} ')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:16:43.082014Z","iopub.execute_input":"2024-06-14T19:16:43.082295Z","iopub.status.idle":"2024-06-14T19:16:43.097080Z","shell.execute_reply.started":"2024-06-14T19:16:43.082270Z","shell.execute_reply":"2024-06-14T19:16:43.096140Z"},"trusted":true},"execution_count":null,"outputs":[]}]}