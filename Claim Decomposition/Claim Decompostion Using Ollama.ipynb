{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8677494,"sourceType":"datasetVersion","datasetId":5201556}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T22:13:22.547971Z","iopub.execute_input":"2024-06-12T22:13:22.548815Z","iopub.status.idle":"2024-06-12T22:13:23.581133Z","shell.execute_reply.started":"2024-06-12T22:13:22.548769Z","shell.execute_reply":"2024-06-12T22:13:23.580270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ollama for subquery generation on training set\n- same code is extended for subquery generation on the validation and test set","metadata":{}},{"cell_type":"code","source":"#Download ollama\n!curl -fsSL https://ollama.com/install.sh | sh\nimport subprocess\nprocess = subprocess.Popen(\"ollama serve\", shell=True) #runs on a different thread\n#Download model\n!ollama pull llama3\n!pip install ollama","metadata":{"execution":{"iopub.status.busy":"2024-06-12T22:13:23.583188Z","iopub.execute_input":"2024-06-12T22:13:23.583702Z","iopub.status.idle":"2024-06-12T22:14:29.353688Z","shell.execute_reply.started":"2024-06-12T22:13:23.583668Z","shell.execute_reply":"2024-06-12T22:14:29.352565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport ollama\n\n# Define the function to decompose claims into subqueries using Ollama\ndef decompose_claim(claim):\n    messages = [\n        {\n            'role': 'user',\n            'content': f\"Decompose the following claim into subquestions and return only the concatenated subqueries with nothing else in the response:\\n\\n{claim}\\n\\n\",\n        },\n    ]\n    response = ollama.chat(model='llama3', messages=messages)\n    \n    return response['message']['content']\n\n# Load the JSON data\ninput_file_path = '/kaggle/input/quantemp-training/train_claims_quantemp.json'\noutput_file_path = '/kaggle/working/ollama-decomposed-train.json'\n\nwith open(input_file_path, 'r') as file:\n    data = json.load(file)\n\nprocessed_data = data\n# Iterate over each entry, decompose the claim and add subqueries\niteration_count = 0\n# Iterate over each entry, decompose the claim and add subqueries\nfor idx, entry in enumerate(processed_data, start=0):\n    claim = entry['claim']\n    subqueries = decompose_claim(claim)\n    entry['subqueries'] = subqueries\n\n    # Print subqueries after every 500 iterations\n    if (idx + 1) % 500 == 0:\n        iteration_count += 1\n        print(f\"Iteration {iteration_count * 500}: Subqueries added: \",subqueries)\n\n# Save the updated data back to a new JSON file\nwith open(output_file_path, 'w') as file:\n    json.dump(processed_data, file, indent=4)\n\nprint(\"Subqueries added and JSON file updated successfully.\")\n","metadata":{},"execution_count":null,"outputs":[]}]}